#!/usr/bin/env python3
"""
Grading runner script for agent patch testing.

This script:
1. Creates a copy of the git repo at baseline commit in /tmp
2. Applies test.patch to this repo (tests should fail)
3. Applies agent.patch to this repo (tests should pass)
4. Generates JUnit XML report at /tmp/grading_results.xml
"""

import json
import logging
import os
import socket
import subprocess
import sys
import threading
import time
import uuid
from pathlib import Path

from packaging import version

from .utils import merge_junits

logger = logging.getLogger(__name__)

class GradingRunner:
    """Handles the grading workflow for agent patch testing."""

    def __init__(
        self,
        base: str,
        test: str,
        golden: str,
        playwright_test_files: list[str] | None = None,
        mocha_test_files: list[str] | None = None, # Warning: ignored for now
        test_files: list[str] | None = None,
        test_patch_path: str = "/home/root/test.patch",
        golden_patch_path: str = "/home/root/golden.patch",
        only_server: bool = False,
    ):
        """
        Initialize the grading runner.

        Args:
            base: The baseline branch name (preferred)
            test: The test branch name (optional, for logging)
            golden: The golden branch name (optional, for logging)
            test_files: List of test files to run
            test_patch_path: Path to the test patch file (default: /home/root/test.patch)
            golden_patch_path: Path to the golden patch file (default: /home/root/golden.patch)
            working_dir: Working directory for grading (default: /tmp/grading_workspace)
        """
        # Determine what to use - branches take precedence
        self.use_base = base
        self.use_test = test
        self.use_golden = golden
        self.test_patch_path = test_patch_path
        self.golden_patch_path = golden_patch_path
        self.test_files = test_files or []
        self.only_server = only_server
        self.grade_working_dir = "/tmp/grading_workspace_" + str(uuid.uuid4())
        self.original_repo_path = os.environ.get("REPO_PATH", "/home/ubuntu/repo")
        
        # Store references to server process and threads for cleanup
        self.server_process = None
        self.server_threads = []
        self.stop_threads = threading.Event()


    def _format_junit_xml(self, test_name: str, message: str, stdout: str, stderr: str) -> str:
        return f"""<?xml version="1.0" encoding="UTF-8"?>
<testsuites>
  <testsuite name="{test_name}" tests="1" failures="1" errors="0" skipped="0">
    <testcase classname="{test_name}" name="test{test_name}" time="0.0">
      <failure type="TestFailure">\n{message}\n</failure>
      <system-out>\n{stdout}\n</system-out>
      <system-err>\n{stderr}\n</system-err>
    </testcase>
  </testsuite>
</testsuites>"""

    def run_tests(self) -> str:
        """
        Run tests and return JUnit XML output.
        
        CUSTOMIZE THIS METHOD for your test framework.
        
        The method should:
        1. Execute tests specified in self.test_files
        2. Generate JUnit XML output
        3. Return the XML content as a string
        
        Examples:
        
        === JEST (Node.js/TypeScript) ===
        command = f"yarn test -- --runInBand --verbose {' '.join(self.test_files)}"
        xml_file = "jest_results.xml"
        
        === PYTEST (Python) ===
        command = f"pytest --junit-xml=pytest_results.xml {' '.join(self.test_files)}"
        xml_file = "pytest_results.xml"
        
        === JUNIT (Java) ===
        command = f"mvn test -Dtest={','.join(self.test_files)}"
        xml_file = "target/surefire-reports/TEST-*.xml"
        
        === GTEST (C++) ===
        command = f"./test_runner --gtest_output=xml:gtest_results.xml --gtest_filter={':'.join(self.test_files)}"
        xml_file = "gtest_results.xml"
        
        === CARGO TEST (Rust) ===
        command = f"cargo test -- --format junit > cargo_results.xml"
        xml_file = "cargo_results.xml"
        """
        logger.info(f"Running tests in {self.grade_working_dir}")
        
        # [CUSTOMIZE] Set your test command here
        test_command = "[TEST_COMMAND] " + " ".join(self.test_files)
        
        result = subprocess.run(
            ["sudo", "-u", "ubuntu", "bash", "-lc", test_command],
            cwd=Path(self.grade_working_dir),
            capture_output=True,
            text=True,
        )
        
        logger.info(f"Tests completed with code: {result.returncode}")
        logger.info(f"Test output: {result.stdout}")
        logger.info(f"Test error: {result.stderr}")
        
        # [CUSTOMIZE] Set your test results XML file path
        xml_file = "[TEST_RESULTS_XML_FILE]"
        
        with open(Path(self.grade_working_dir) / xml_file) as f:
            return f.read()

    def _wait_for_server(self, host: str = "localhost", port: int = 3000, timeout: int = 600) -> bool:
        """Wait for server to be ready by checking if port is listening."""
        start_time = time.time()
        while time.time() - start_time < timeout:
            try:
                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                    sock.settimeout(1)
                    result = sock.connect_ex((host, port))
                    if result == 0:
                        logger.info(f"Server is ready on {host}:{port}")
                        return True
            except Exception:
                pass
            time.sleep(1)
        return False

    def _cleanup_generated_files(self):
        """
        Remove generated files that may interfere with the build.
        
        CUSTOMIZE THIS METHOD for your project's build artifacts.
        
        This is useful when agents generate intermediate files that conflict
        with the build system (e.g., compiling TypeScript to JavaScript when
        using a bundler, or generating .pyc files that should be cleaned).
        
        Examples:
        
        === TYPESCRIPT â†’ JAVASCRIPT ===
        Remove .js files generated from .ts files in specific directories
        
        === PYTHON ===
        find . -type f -name "*.pyc" -delete
        find . -type d -name "__pycache__" -delete
        
        === JAVA ===
        find . -type f -name "*.class" -delete
        
        === C++ ===
        rm -rf build/ *.o *.out
        
        Leave empty if no cleanup needed.
        """
        logger.info("Cleaning up generated files")
        
        # [CUSTOMIZE] Add your cleanup logic here
        # Example: self._cleanup_typescript_js_files()
        
        pass  # Remove this when implementing cleanup

    def _needs_server_start(self) -> bool:
        """Check if current version needs server to be started for tests."""
        try:
            # Read package.json version from the working directory
            package_json_path = Path(self.grade_working_dir) / "package.json"
            with open(package_json_path) as f:
                package_data = json.load(f)
            
            current_version = package_data.get("version", "0.0.0")
            
            # At least version 0.66.2 (may be larger, im using attachmentsvalidation_baseline as a reference)
            threshold_version = "0.66.2"
            
            needs_server = version.parse(current_version) <= version.parse(threshold_version)
            logger.info(f"Current version: {current_version}, Server needed: {needs_server} (version {'<=' if needs_server else '>'} {threshold_version})")
            return needs_server
            
        except Exception as e:
            logger.warning(f"Error checking package.json version: {e}, assuming server needed")
            return True

    def run_grading(self) -> tuple[bool, dict]:
        """Run the complete grading workflow."""
        logger.info("Starting grading workflow")
        # Step 1: Copy original repo to working dir
        logger.info(f"Copying original repo to {self.grade_working_dir}")
        subprocess.run(["sudo", "-u", "ubuntu", "cp", "-r", self.original_repo_path, self.grade_working_dir], check=True)
        logger.info(f"Copied original repo to {self.grade_working_dir}")

        # Step 2: apply test patch
        logger.info(f"Applying test patch to {self.grade_working_dir}")
        with open(self.test_patch_path) as f:
            subprocess.run(["sudo", "-u", "ubuntu", "git", "apply"], check=True, cwd=self.grade_working_dir, input=f.read().encode("utf-8"))
        logger.info(f"Applied test patch to {self.grade_working_dir}")

        # Step 3: Clean up any generated files that might interfere with the build
        self._cleanup_generated_files()

        # Step 4: compile the project (should work if the agent code compiles)
        logger.info(f"Compiling project in {self.grade_working_dir}")
        
        # [CUSTOMIZE] Set your build command
        # Examples:
        #   Node.js: "NODE_OPTIONS=\"--max-old-space-size=4096\" yarn build" or "npm run build"
        #   Python: (often no build step needed)  or "python setup.py build"
        #   Java: "mvn package -DskipTests"
        #   Rust: "cargo build --release"
        #   C++: "cd build && cmake .. && make"
        build_command = "[BUILD_COMMAND]"
        
        # Run build and stream output to stderr in real-time
        build_process = subprocess.Popen(
            ["sudo", "-u", "ubuntu", "bash", "-lc", build_command],
            cwd=self.grade_working_dir,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
        )
        
        # Collect output for error reporting while streaming
        build_output = []
        
        def stream_build_stdout():
            """Stream stdout to stderr while collecting for error reporting."""
            for line in build_process.stdout:
                sys.stderr.write(line)
                sys.stderr.flush()
                build_output.append(line)
        
        def stream_build_stderr():
            """Stream stderr to stderr while collecting for error reporting."""
            for line in build_process.stderr:
                sys.stderr.write(line)
                sys.stderr.flush()
                build_output.append(line)
        
        # Start streaming threads
        stdout_thread = threading.Thread(target=stream_build_stdout)
        stderr_thread = threading.Thread(target=stream_build_stderr)
        
        stdout_thread.start()
        stderr_thread.start()
        
        # Wait for threads to complete
        stdout_thread.join()
        stderr_thread.join()
        
        # Wait for the process to complete
        build_result_code = build_process.wait()
        
        # Check exit code
        if build_result_code != 0:
            # Format compile error as JUnit XML
            xml_content = self._format_junit_xml("AgentPatchCompiles", "Agent patch compilation failed", "".join(build_output), "")
            logger.info(f"Compilation failed with exit code {build_result_code}")
            return False, {"junit": xml_content}
        
        logger.info(f"Compiled project successfully in {self.grade_working_dir}")

        # Step 5: Reset test database and run migrations
        db_name = os.environ.get("TEST_DB_NAME", "test_db")
        logger.info(f"Resetting {db_name} database via psql")
        drop_cmd = (
            f"PGPASSWORD=ubuntu psql -h localhost -U ubuntu -c \"DROP DATABASE IF EXISTS {db_name};\""
        )
        create_cmd = (
            f"PGPASSWORD=ubuntu psql -h localhost -U ubuntu -c \"CREATE DATABASE {db_name};\""
        )
        
        # [CUSTOMIZE] Set your migration command (or set to None if not needed)
        # Examples:
        #   Node.js/Sequelize: "export NODE_ENV=test && yarn db:migrate"
        #   Django: "python manage.py migrate --noinput"
        #   Rails: "bundle exec rake db:migrate"
        #   Prisma: "npx prisma migrate deploy"
        migrate_cmd = "[MIGRATION_COMMAND]"  # Or None if no migrations

        drop_res = subprocess.run(["bash", "-lc", drop_cmd], cwd=self.grade_working_dir, capture_output=True, text=True)
        logger.info(f"Drop DB exit: {drop_res.returncode}\n{drop_res.stdout}\n{drop_res.stderr}")
        create_res = subprocess.run(["bash", "-lc", create_cmd], cwd=self.grade_working_dir, capture_output=True, text=True)
        logger.info(f"Create DB exit: {create_res.returncode}\n{create_res.stdout}\n{create_res.stderr}")
        migrate_res = subprocess.run(["sudo", "-u", "ubuntu", "bash", "-lc", migrate_cmd], cwd=self.grade_working_dir, capture_output=True, text=True)
        logger.info(f"Migrate exit: {migrate_res.returncode}\n{migrate_res.stdout}\n{migrate_res.stderr}")

        # Step 6: Conditionally start server based on version
        if self._needs_server_start():
            # [CUSTOMIZE] Set your server start command
            # Examples:
            #   Node.js: "yarn start" or "npm start"
            #   Django: "python manage.py runserver 3000"
            #   Flask: "flask run --port=3000"
            #   Spring Boot: "java -jar target/app.jar --server.port=3000"
            server_start_cmd = "[SERVER_START_COMMAND]"
            
            logger.info(f"Starting server with {server_start_cmd} (required for this version)")
            self.server_process = subprocess.Popen(
                ["sudo", "-u", "ubuntu", "bash", "-lc", server_start_cmd],
                cwd=self.grade_working_dir,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
            )
            
            # Wait for server to be ready
            if not self._wait_for_server():
                logger.error("Server failed to start within timeout")
                if self.server_process:
                    self.server_process.terminate()
                xml_content = self._format_junit_xml("ServerStart", "Server failed to start within timeout", "", "")
                return False, {"junit": xml_content}
        else:
            logger.info("Skipping server start (not required for this version)")

        # Step 7: Run tests
        junit_xmls = [self.run_tests()]
        
        # Cleanup: Stop server if it was started
        if self.server_process:
            logger.info("Stopping server")
            self.server_process.terminate()
            try:
                self.server_process.wait(timeout=10)
            except subprocess.TimeoutExpired:
                self.server_process.kill()

        merged_junit, full_success = merge_junits(junit_xmls)
        return full_success, {"junit": merged_junit}